[<- Back](../README.md)

[ğŸ Home](../../../README.md) > [ğŸ“–Tech Blog](../../README.md) > [ğŸ“·Computer Vision](../README.md) > \[Paper Review\] Restormer: Efficient Transformer for High-Resolution Image Restoration

### \[Paper Review\]
# Restormer: Efficient Transformer for High-Resolution Image Restoration
2023-04-12

-----

[Paper](https://arxiv.org/pdf/2111.09881.pdf) | [Code](https://github.com/swz30/Restormer)

-----

## ëª©ì°¨
1. [Diffusion Model](#1-diffusion-model)
2. [Range-Null Space Decomposition](#2-range-null-space-decomposition)
3. [Denoising Diffusion Null-Space Model](#3-denoising-diffusion-null-space-model)
4. [DDNM+](#4-ddnm)

-----

Convolutionì€ local connectivityì™€ translation equivariance
- limited receptive field -> preventing it from modeling long-range pixel dependencies
- static weights at inference -> cannot flexibly adapt to the input content
ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ self-attention(SA) ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©

## 1. Diffusion Model
### 1.1. Forward Process
DDPMì€ $T$-stepì˜ forward processì™€ $T$-stepì˜ reverse processë¡œ ì •ì˜ëœë‹¤. Forward processëŠ” ì²œì²œíˆ random noiseë¥¼ ë°ì´í„°ì— ë”í•˜ëŠ” ê³¼ì •ì´ë©°, reverse processëŠ” noiseë¡œë¶€í„° data sampleì„ ë³µì›í•˜ëŠ” ê³¼ì •ì´ë‹¤.

Scale factorì¸ $\beta_t$ì™€ ì´ì „ ìƒíƒœì¸ $\boldsymbol{x}_{t-1}$ì— ëŒ€í•˜ì—¬, í˜„ì¬ ìƒíƒœì¸ $\boldsymbol{x}_t$ë¡œì˜ forward processëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})
=\mathcal{N}(\boldsymbol{x}_t;\sqrt{1-\beta_t}\boldsymbol{x}_{t-1}, \beta_t\boldsymbol{I})
$$
ì´ë¥¼ í˜„ì¬ ìƒíƒœì¸ $\boldsymbol{x_t}$ì˜ ê´€ì ì—ì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\boldsymbol{x_t}
=\sqrt{1-\beta_t}\boldsymbol{x}_{t-1}+\sqrt{\beta_t}\boldsymbol{\epsilon},\quad\boldsymbol{\epsilon}\sim\mathcal{N}(0,\boldsymbol{I})
$$
ì—¬ê¸°ì„œ reparametrization trickì„ ì‚¬ìš©í•˜ë©´,
$$
q(\boldsymbol{x}_t|\boldsymbol{x}_0)
=\mathcal{N}(\boldsymbol{x}_t;\sqrt{\bar{\alpha}_t}\boldsymbol{x}_0,(1-\bar{\alpha}_t)\boldsymbol{I})\\
{\rm with}\quad\alpha_t=1-\beta_t,\quad\bar{\alpha}_t=\prod_{i=1}^{t}\alpha_i
$$
ê°€ ë¨ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.

### 1.2. Reverse Process
Reverse processëŠ” í˜„ì¬ ìƒíƒœ $\boldsymbol{x}_t$ë¡œë¶€í„° ì´ì „ ìƒíƒœì¸ $\boldsymbol{x}_{t-1}$ì„ ì–»ê¸° ìœ„í•˜ì—¬ posterior distribution $p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0)$ì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ë‹¤. Forward processì˜ ìˆ˜ì‹ê³¼ Bayes ì •ë¦¬ë¥¼ ì´ìš©í•˜ë©´,
$$
p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)
=q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})\frac{q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_0)}{q(\boldsymbol{x}_t|\boldsymbol{x}_0)}
=\mathcal{N}(\boldsymbol{x}_{t-1};\boldsymbol{\mu}_t(\boldsymbol{x}_t,\boldsymbol{x}_0), \sigma_t^2\boldsymbol{I})\\
{\rm with}\quad\boldsymbol{\mu}_t(\boldsymbol{x}_t,\boldsymbol{x}_0)=\frac{1}{\sqrt{\alpha_t}}\left(\boldsymbol{x}_t-\boldsymbol{\epsilon}\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\right),\quad
\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t
$$
ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

DDPMì€ neural network $\mathcal{Z}_{\boldsymbol{\theta}}$ë¥¼ ì´ìš©í•˜ì—¬ time-step $t$ì˜ noise $\boldsymbol{\epsilon}$ì„ ì˜ˆì¸¡í•˜ë©°, ì˜ˆì¸¡í•œ noiseëŠ” $\boldsymbol{\epsilon}_t=\mathcal{Z}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)$ë¡œ í‘œê¸°í•œë‹¤. DDPMì—ì„œëŠ” ë°ì´í„°ì…‹ì—ì„œ cleaní•œ ì´ë¯¸ì§€ $\boldsymbol{x}_0$ê³¼ noise $\boldsymbol{\epsilon}\sim\mathcal{N}(0,\boldsymbol{I})$, time-step $t$ë¥¼ ëœë¤í•˜ê²Œ ë½‘ì•„ neural network $\mathcal{Z}_{\boldsymbol{\theta}}$ì˜ parameterì¸ $\boldsymbol{\theta}$ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ gradient stepìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.
$$
\nabla_{\boldsymbol{\theta}}||\boldsymbol{\epsilon}-\mathcal{Z}_{\boldsymbol{\theta}}(\sqrt{\bar{\alpha}_t}\boldsymbol{x}_0+\boldsymbol{\epsilon}\sqrt{1-\bar{\alpha}_t},t)||_2^2
$$
DDPMì—ì„œëŠ” í•™ìŠµëœ neural network $\mathcal{Z}_{\boldsymbol{\theta}}$ë¥¼ ì´ìš©í•˜ì—¬ random noises $\boldsymbol{x}_T\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$ë¡œë¶€í„°, $p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)$ë¥¼ ì´ìš©í•˜ì—¬ $\boldsymbol{x}_{t-1}$ì„ ë°˜ë³µì ìœ¼ë¡œ samplingí•˜ê²Œ ë˜ë©´ clean images $\boldsymbol{x}_0\sim q(\boldsymbol{x})$ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

-----

## 2. Range-Null Space Decomposition
> **Pseudo-inverse**ë€, ì£¼ì–´ì§„ linear operator $\boldsymbol{A}\in\mathbb{R}^{d\times D}$ì— ëŒ€í•˜ì—¬, $\boldsymbol{A}\boldsymbol{A}^\dagger \boldsymbol{A}\equiv\boldsymbol{A}$ë¥¼ ë§Œì¡±í•˜ëŠ” $\boldsymbol{A}^\dagger\in\mathbb{R}^{D\times d}$ë¥¼ ë§í•œë‹¤.

$\boldsymbol{A}\boldsymbol{A}^\dagger \boldsymbol{A}\boldsymbol{x}\equiv \boldsymbol{A}\boldsymbol{x}$ì´ë¯€ë¡œ $\boldsymbol{A}^\dagger \boldsymbol{A}$ëŠ” sample $\boldsymbol{x}\in\mathbb{R}^{D\times 1}$ë¥¼ $\boldsymbol{A}$ì˜ range-spaceë¡œ projectioní•˜ëŠ” operatorë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ $(\boldsymbol{I}-\boldsymbol{A}^\dagger\boldsymbol{A})$ëŠ” sample $\boldsymbol{x}$ë¥¼ $\boldsymbol{A}$ì˜ null-spaceë¡œ projectioní•˜ëŠ” operatorê°€ ëœë‹¤.

ë”°ë¼ì„œ, ì„ì˜ì˜ sample $\boldsymbol{x}$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ $\boldsymbol{A}$ì˜ range-spaceì™€ null-spaceì˜ ë‘ ê°€ì§€ ì„±ë¶„ìœ¼ë¡œ decompositionë  ìˆ˜ ìˆë‹¤.
$$
\boldsymbol{x}\equiv\boldsymbol{A}^\dagger\boldsymbol{A}\boldsymbol{x}+(\boldsymbol{I}-\boldsymbol{A}^\dagger\boldsymbol{A})\boldsymbol{x}
$$

-----

## 3. Denoising Diffusion Null-Space Model
### 3.1. Image Restoration & Null-Space
ë¨¼ì €, ground-truth ì´ë¯¸ì§€ $\boldsymbol{x}\in\mathbb{R}^{D\times 1}$ì™€ linear degradation operator $\boldsymbol{A}\in\mathbb{R}^{d\times D}$, degraded ì´ë¯¸ì§€ $\boldsymbol{y}\in\mathbb{R}^{d\times 1}$ì˜ ê´€ê³„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
$$
\boldsymbol{y}=\boldsymbol{A}\boldsymbol{x}
$$
ì£¼ì–´ì§„ ground-truth ì´ë¯¸ì§€ $\boldsymbol{y}$ì— ëŒ€í•˜ì—¬, noise-free image restoration ë¬¸ì œëŠ” ì•„ë˜ì˜ constraintsë¥¼ ê°–ëŠ” ì´ë¯¸ì§€ $\hat{\boldsymbol{x}}\in\mathbb{R}^{D\times 1}$ì„ ì°¾ëŠ” ë¬¸ì œì´ë‹¤.
$$
{\rm Consistency:}\quad \boldsymbol{A}\hat{\boldsymbol{x}}\equiv\boldsymbol{y},\quad
{\rm Realness:}\quad \hat{\boldsymbol{x}}\sim q(\boldsymbol{x})
$$

Consistency constraint $\boldsymbol{A}\hat{\boldsymbol{x}}\equiv\boldsymbol{y}$ë¥¼ ë§Œì¡±í•˜ëŠ” general solution $\hat{\boldsymbol{x}}$ëŠ” $\hat{\boldsymbol{x}}=\boldsymbol{A}^\dagger\boldsymbol{y}+(\boldsymbol{I}-\boldsymbol{A}^\dagger\boldsymbol{A})\bar{\boldsymbol{x}}$ì˜ í˜•íƒœê°€ ëœë‹¤. ì´ ë•Œ, $\boldsymbol{\bar{x}}$ê°€ ë¬´ì—‡ì´ë“  consistency constraintëŠ” ë§Œì¡±í•˜ë¯€ë¡œ realness constraint $\hat{\boldsymbol{x}}\sim q(\boldsymbol{x})$ë¥¼ ë§Œì¡±í•˜ê²Œ í•˜ëŠ” ì ì ˆí•œ $\bar{\boldsymbol{x}}$ë¥¼ ì°¾ì•„ì•¼í•œë‹¤.

### 3.2. Refine Null-Space Iteratively
Reverse diffusion processëŠ” 
$$
\boldsymbol{\mu}_t(\boldsymbol{x}_t, \boldsymbol{x}_0)=\frac{\sqrt{\bar{\alpha}_{t-1}\beta_t}}{1-\bar{\alpha}_t}\boldsymbol{x}_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\boldsymbol{x}_t,\quad
\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t
$$

$$
\boldsymbol{x}_{0|t}=\frac{1}{\sqrt{\bar{\alpha}_t}}(\boldsymbol{x}_t-\mathcal{Z}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\sqrt{1-\bar{\alpha}_t})
$$

> **Algorithm** | Sampling of DDNM
> 1. $\boldsymbol{x}_T\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$
> 2. **for** $t=T, \dots, 1$ **do**
> 3. $\qquad\boldsymbol{x}_{0|t} = \frac{1}{\sqrt{\bar{\alpha}}_t}\left(\boldsymbol{x}_t-\mathcal{Z}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\sqrt{1-\bar{\alpha}_t}\right)$
> 4. $\qquad\hat{\boldsymbol{x}}_{0|t}=\boldsymbol{A}^\dagger\boldsymbol{y}+(\boldsymbol{I}-\boldsymbol{A}^\dagger\boldsymbol{A})\boldsymbol{x}_{0|t}$
> 5. $\qquad\boldsymbol{x}_{t-1}\sim p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\hat{\boldsymbol{x}}_{0|t})$
> 6. **return** $\boldsymbol{x}_0$

### 3.3. Examples of $\boldsymbol{A}$ and $\boldsymbol{A}^\dagger$
- Inpainting: $\boldsymbol{A}$ëŠ” ê°„ë‹¨í•˜ê²Œ mask operatorê°€ ëœë‹¤.
- Colorization: $\boldsymbol{A}$ë¥¼ pixel-wise operatorë¼ê³  ìƒê°í•˜ë©´ ê°ê°ì˜ RGB channel pixel $[r\quad g\quad b]^\top$ì— ëŒ€í•˜ì—¬ grayscale ê°’ì€ $\left[\frac{r}{3}+\frac{g}{3}+\frac{b}{3}\right]$ì´ë¯€ë¡œ $\boldsymbol{A}=\left[\frac{1}{3}\quad\frac{1}{3}\quad\frac{1}{3}\right]$ì´ ë˜ë©°, $\boldsymbol{A}^\dagger=[1\quad 1\quad 1]^\top$ì´ ëœë‹¤.
- Super-resolution: scaleì´ $n$ì´ë¼ê³  í•  ë•Œ, ê°ê°ì˜ $n\times n$ í¬ê¸°ì˜ íŒ¨ì¹˜ì— ëŒ€í•˜ì—¬ $\boldsymbol{A}\in\mathbb{R}^{1\times n^2}$ì€ average-pooling operatorê°€ ë˜ì–´ì•¼í•˜ë¯€ë¡œ $\boldsymbol{A}=\left[\frac{1}{n^2}\quad\dots\quad\frac{1}{n^2}\right]$ì™€ $\boldsymbol{A}^\dagger\in\mathbb{R}^{n^2\times 1}=[1\quad\dots\quad 1]^\top$ê°€ ëœë‹¤.

$\boldsymbol{A}$ê°€ ì—¬ëŸ¬ sub-operationë“¤ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ë©´, $\boldsymbol{A}=\boldsymbol{A}_1\dots\boldsymbol{A}_n$ì¼ ë•Œ, $\boldsymbol{A}$ì˜ pseudo-inverse $\boldsymbol{A}^\dagger=\boldsymbol{A}_n^\dagger\dots\boldsymbol{A}_1^\dagger$ê°€ ëœë‹¤.

-----

## 4. DDNM+
### 4.1. Scaling Range-Space Correction
Noisy image restorationëŠ” additive Gaussian noise $\boldsymbol{n}\in\mathbb{R}^{d\times 1}\sim\mathcal{N}(\boldsymbol{0},\sigma_{\boldsymbol{y}}^2\boldsymbol{I})$ì— ëŒ€í•˜ì—¬ $\boldsymbol{y}=\boldsymbol{A}\boldsymbol{x}+\boldsymbol{n}$ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì— DDNMì„ ì§ì ‘ ì ìš©í•˜ê²Œ ë˜ë©´
$$
\hat{\boldsymbol{x}}_{0|t}=\boldsymbol{A}^\dagger\boldsymbol{y}+(\boldsymbol{I}-\boldsymbol{A}^\dagger\boldsymbol{A})\boldsymbol{x}_{0|t}
=\boldsymbol{x}_{0|t}-\boldsymbol{A}^\dagger(\boldsymbol{A}\boldsymbol{x}_{0|t}-\boldsymbol{A}\boldsymbol{x})+\boldsymbol{A}^\dagger\boldsymbol{n}
$$
ì´ ë˜ë©°, $\boldsymbol{A}^\dagger\boldsymbol{n}\in\mathbb{R}^{D\times 1}$ì€ extra noiseê°€ ëœë‹¤.


### 4.2. Time-Travel Trick

> **Algorithm** | Sampling of DDNM+
> 1. $\boldsymbol{x}_T\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$
> 2. **for** $t=T, \dots, 1$ **do**
> 3. $\qquad L=\min\left\{T-t,l\right\}$
> 4. $\qquad\boldsymbol{x}_{t+L}\sim q(\boldsymbol{x}_{t+L}|\boldsymbol{x}_t)$
> 5. $\qquad$**for** $j=L,\dots,0$ **do**
> 6. $\qquad\qquad\boldsymbol{x}_{0|t+j} = \frac{1}{\sqrt{\bar{\alpha}}_{t+j}}\left(\boldsymbol{x}_{t+j}-\mathcal{Z}_{\boldsymbol {\theta}}(\boldsymbol{x}_{t+j}, t+j)\sqrt{1-\bar{\alpha}_{t+j}}\right)$
> 7. $\qquad\qquad\hat{\boldsymbol{x}}_{0|t+j}=\boldsymbol{x}_{0|t+j}-\boldsymbol{\Sigma}_{t+j}\boldsymbol{A}^\dagger(\boldsymbol{A}\boldsymbol{x}_{0|t+j}-\boldsymbol{y})$
> 8. $\qquad\qquad\boldsymbol{x}_{t+j-1}\sim p(\boldsymbol{x}_{t+j-1}|\boldsymbol{x}_{t+j},\hat{\boldsymbol{x}}_{0|t+j})$
> 9. **return** $\boldsymbol{x}_0$
